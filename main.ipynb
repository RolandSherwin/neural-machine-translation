{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python391jvsc74a57bd0c840b9d0f3e362d11319d49b9d4102ebc5f5d2914c747b791c8b3f55d8a640ac",
   "display_name": "Python 3.9.1 64-bit ('ml': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "597ef2c4e330346690cfc3eee48a64f4a5787db7dc7ff65ed373f1569f4e13ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from babel.dates import format_date\n",
    "from faker import Faker\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as tfl"
   ]
  },
  {
   "source": [
    "# Dataset Generator\n",
    "\n",
    "The data for this model is generated using the **Faker** python library. It's a simple tool that allows us to create fake data. The data is then preprocessed such that each character in an example is a one-hot vector."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_dataset(m):\n",
    "    \"\"\"Generates a dataset with X, Y where X is the human readable date and Y is the machine readable ISO standard. \n",
    "    It also returns 3 dictionaries which are used to encode the dataset.\n",
    "    \"\"\"\n",
    "    formats = ['short','medium', 'full', 'full', 'full', 'full', 'full', 'full', 'full',\n",
    "            'd MMM YYY', 'd MMMM YYY', 'dd MMM YYY', 'd MMM, YYY', 'd MMMM, YYY',\n",
    "            'dd, MMM YYY', 'd MM YY', 'd MMMM YYY', 'MMMM d YYY', 'MMMM d, YYY', 'dd.MM.YY']\n",
    "    fake = Faker()\n",
    "    Faker.seed(42)\n",
    "\n",
    "    dataset = []\n",
    "    # Create a set which will store all the unique characters. Adding\n",
    "    human_vocab = set(('<unk>', '<pad>')) \n",
    "    machine_vocab = set() # set(('<pad>',)) \n",
    "    error_counter = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        date = fake.date_object()\n",
    "        try: \n",
    "            human_readable = format_date(date, format=random.choice(formats), locale='en_US')\n",
    "            human_readable = human_readable.lower()\n",
    "            human_readable = human_readable.replace(',','')\n",
    "            machine_readable = date.isoformat()\n",
    "            \n",
    "            dataset.append((human_readable, machine_readable))\n",
    "            human_vocab.update(tuple(human_readable))\n",
    "            machine_vocab.update(tuple(machine_readable))\n",
    "\n",
    "        except AttributeError as e:\n",
    "            error_counter += 1\n",
    "            print(\"Error while generating dataset, count: \", error_counter)\n",
    "        \n",
    "    human_vocab = dict(zip( sorted(human_vocab), list(range(len(human_vocab)))  ))\n",
    "    machine_vocab = dict(zip( sorted(machine_vocab), list(range(len(machine_vocab))) ))\n",
    "    machine_vocab_inv = {v:k for k,v in machine_vocab.items()}\n",
    "\n",
    "    return dataset, human_vocab, machine_vocab, machine_vocab_inv\n",
    "\n",
    "def preprocess_dataset(dataset, human_vocab, machine_vocab, Tx, Ty):\n",
    "    \"\"\"Encodes all the characters in the dataset to one-hot vectors.\n",
    "    \"\"\"\n",
    "    X, Y = zip(*dataset) #unpack the list\n",
    "\n",
    "    # Get index of each character as defined in the vocab\n",
    "    X = np.array([string_to_index(i, Tx, human_vocab) for i in X]) \n",
    "    Y = np.array([string_to_index(i, Ty, machine_vocab) for i in Y])\n",
    "\n",
    "    # Convert the indices to One-hot vectors\n",
    "    Xoh = np.array(list(map(lambda x: tf.keras.utils.to_categorical(x, num_classes=len(human_vocab)), X)))\n",
    "    Yoh = list(map(lambda x: tf.keras.utils.to_categorical(x, num_classes=len(machine_vocab)), Y))\n",
    "\n",
    "    return X, Y, Xoh, np.array(Yoh)\n",
    "\n",
    "def string_to_index(string, length, vocab):\n",
    "    \"\"\"Converts the given string to their index values defined by the vocab. It pads the string if it is less\n",
    "    than the given length or truncates the string if it is above the given length.\n",
    "    \"\"\"\n",
    "    string = string.lower()\n",
    "    string = string.replace(',','')\n",
    "\n",
    "    # truncate\n",
    "    if  len(string) > length:\n",
    "        string = string[:length]\n",
    "\n",
    "    indices = []\n",
    "    for x in list(string):\n",
    "\n",
    "        index = vocab.get(x, '<unk>')\n",
    "        indices.append(index)\n",
    "    \n",
    "    # append\n",
    "    if len(indices) < length:\n",
    "        indices += [vocab['<pad>']] * (length - len(indices))\n",
    "\n",
    "    return indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(25000, 30) (25000, 10) (25000, 30, 37) (25000, 10, 11)\n(5000, 30) (5000, 10) (5000, 30, 37) (5000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "m_total = 30000\n",
    "m = 25000\n",
    "m_val = m_total - m\n",
    "Tx = 30\n",
    "Ty = 10\n",
    "\n",
    "dataset, human_vocab, machine_vocab, machine_vocab_inv = generate_dataset(m_total)\n",
    "X_total, Y_total, Xoh_total, Yoh_total = preprocess_dataset(dataset, human_vocab, machine_vocab, Tx, Ty)\n",
    "\n",
    "# Training set\n",
    "X, X_val = X_total[:m], X_total[m:]\n",
    "Xoh, Xoh_val = Xoh_total[:m], Xoh_total[m:]\n",
    "# Validation Set\n",
    "Y, Y_val = Y_total[:m], Y_total[m:]\n",
    "Yoh, Yoh_val = Yoh_total[:m], Yoh_total[m:]\n",
    "\n",
    "print(X.shape, Y.shape, Xoh.shape, Yoh.shape)\n",
    "print(X_val.shape, Y_val.shape, Xoh_val.shape, Yoh_val.shape)"
   ]
  },
  {
   "source": [
    "# Attention Model\n",
    "\n",
    "In a sequence-to-sequence model, the traditional approach is to run the input through a series of RNN/GRU/LSTM units which **memorizes** the entire input sequence. This is the encoding part of the network, it outputs a single vector which should then pass the entire information about the input sequence to a decoder which decodes to give the output sequence.\n",
    "\n",
    "As we can see this of model fails when a very long sequence is given since its **hard to memorize the entire input**. Thus we use a much powerful model called **Attention Model.** In an attention model, to generate the first output word, we just need to look at the first few words of the input and we don't need to look very deep in the input sequence. Thus we can compute some **attention weights, $\\alpha^{<t, t'>}$** which tells how **much attention we need to pay** for a specific input word $t'$ when generating a specific output word $t$.\n",
    "\n",
    "![](./images/attention_network.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These layers are defined globally because their weights are re-used druing each timestep.\n",
    "\n",
    "repeat_vector = tfl.RepeatVector(Tx)\n",
    "concatenate = tfl.Concatenate(axis = -1)\n",
    "dense_layer1 = tfl.Dense(10, activation='tanh')\n",
    "dense_layer2 = tfl.Dense(1, activation='relu')\n",
    "softmax_layer = tfl.Softmax(axis=-1)\n",
    "dot = tfl.Dot(axes=1)\n",
    "\n",
    "n_a = 32 # number of units for the input LSTMs\n",
    "n_s = 64 # number of units for the output LSTMs\n",
    "output_LSTM = tfl.LSTM(n_s, return_state = True)\n",
    "output_layer = tfl.Dense(len(machine_vocab), activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(a, s_prev):\n",
    "    \"\"\"Given a single timestep/character, this function calculates how much context\n",
    "    it should take from its neighbouring timesteps/characters.\n",
    "    The s_prev is the output of a single LSTM, while 'a' is the output of all the input   \n",
    "    timesteps(Tx). Thus s_prev is repeated Tx times. Then both are concatenated and \n",
    "    passed through 2 dense layers. \n",
    "\n",
    "    Finally they are given to a softmax layer to calculate the Attention Weight, alpha.\n",
    "    This tells us how much amount of weight it should give each of the input activation.\n",
    "    Thus its normalized using softmax such that all their weights add up to 1.\n",
    "\n",
    "    Then we can finally apply the weights to the activations using the dot layer. This \n",
    "    gives us the amount of attention/context per activation (i.e. input timestep/char).\n",
    "    \"\"\"\n",
    "    s_prev = repeat_vector(s_prev)\n",
    "    concat = concatenate([a, s_prev])\n",
    "    e = dense_layer1(concat)\n",
    "    e = dense_layer2(e)\n",
    "    alpha = softmax_layer(e)\n",
    "    context = dot([alpha, a])\n",
    "\n",
    "    return context\n",
    "\n",
    "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
    "    \"\"\"The activations of all the input timesteps/characters are calculated\n",
    "    using the first Bidirectional LSTM. We can then use these activations to find the\n",
    "    context for a single output at time, while simultaneously finding its output\n",
    "    prediction.\n",
    "    \"\"\"\n",
    "    # Defining the inputs\n",
    "    X = tfl.Input(shape=(Tx, human_vocab_size))\n",
    "    s0 = tfl.Input(shape=(n_s,), name='s0')\n",
    "    c0 = tfl.Input(shape=(n_s,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    # This first LSTM calculates the activations of all the input timesteps/characters\n",
    "    a = tfl.Bidirectional(tfl.LSTM(units=n_a, return_sequences=True))(X)\n",
    "\n",
    "    # Then to predict each out the output character, we first find the context\n",
    "    # using the above attention function and then pass it through the output LSTM\n",
    "    # and a softmax layer to produce one output at a time.\n",
    "    for t in range(Ty):\n",
    "        context = attention(a, s)\n",
    "        s, _, c = output_LSTM(inputs=context, initial_state=[s,c])\n",
    "        out = output_layer(s)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[X,s0,c0], outputs=outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx = 30\n",
    "Ty = 10\n",
    "n_a = 32\n",
    "n_s = 64\n",
    "model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 30, 37)]     0                                            \n__________________________________________________________________________________________________\ns0 (InputLayer)                 [(None, 64)]         0                                            \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 30, 64)       17920       input_1[0][0]                    \n__________________________________________________________________________________________________\nrepeat_vector (RepeatVector)    (None, 30, 64)       0           s0[0][0]                         \n                                                                 lstm[0][0]                       \n                                                                 lstm[1][0]                       \n                                                                 lstm[2][0]                       \n                                                                 lstm[3][0]                       \n                                                                 lstm[4][0]                       \n                                                                 lstm[5][0]                       \n                                                                 lstm[6][0]                       \n                                                                 lstm[7][0]                       \n                                                                 lstm[8][0]                       \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 30, 128)      0           bidirectional[0][0]              \n                                                                 repeat_vector[0][0]              \n                                                                 bidirectional[0][0]              \n                                                                 repeat_vector[1][0]              \n                                                                 bidirectional[0][0]              \n                                                                 repeat_vector[2][0]              \n                                                                 bidirectional[0][0]              \n                                                                 repeat_vector[3][0]              \n                                                                 bidirectional[0][0]              \n                                                                 repeat_vector[4][0]              \n                                                                 bidirectional[0][0]              \n                                                                 repeat_vector[5][0]              \n                                                                 bidirectional[0][0]              \n                                                                 repeat_vector[6][0]              \n                                                                 bidirectional[0][0]              \n                                                                 repeat_vector[7][0]              \n                                                                 bidirectional[0][0]              \n                                                                 repeat_vector[8][0]              \n                                                                 bidirectional[0][0]              \n                                                                 repeat_vector[9][0]              \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 30, 10)       1290        concatenate[0][0]                \n                                                                 concatenate[1][0]                \n                                                                 concatenate[2][0]                \n                                                                 concatenate[3][0]                \n                                                                 concatenate[4][0]                \n                                                                 concatenate[5][0]                \n                                                                 concatenate[6][0]                \n                                                                 concatenate[7][0]                \n                                                                 concatenate[8][0]                \n                                                                 concatenate[9][0]                \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 30, 1)        11          dense[0][0]                      \n                                                                 dense[1][0]                      \n                                                                 dense[2][0]                      \n                                                                 dense[3][0]                      \n                                                                 dense[4][0]                      \n                                                                 dense[5][0]                      \n                                                                 dense[6][0]                      \n                                                                 dense[7][0]                      \n                                                                 dense[8][0]                      \n                                                                 dense[9][0]                      \n__________________________________________________________________________________________________\nsoftmax (Softmax)               (None, 30, 1)        0           dense_1[0][0]                    \n                                                                 dense_1[1][0]                    \n                                                                 dense_1[2][0]                    \n                                                                 dense_1[3][0]                    \n                                                                 dense_1[4][0]                    \n                                                                 dense_1[5][0]                    \n                                                                 dense_1[6][0]                    \n                                                                 dense_1[7][0]                    \n                                                                 dense_1[8][0]                    \n                                                                 dense_1[9][0]                    \n__________________________________________________________________________________________________\ndot (Dot)                       (None, 1, 64)        0           softmax[0][0]                    \n                                                                 bidirectional[0][0]              \n                                                                 softmax[1][0]                    \n                                                                 bidirectional[0][0]              \n                                                                 softmax[2][0]                    \n                                                                 bidirectional[0][0]              \n                                                                 softmax[3][0]                    \n                                                                 bidirectional[0][0]              \n                                                                 softmax[4][0]                    \n                                                                 bidirectional[0][0]              \n                                                                 softmax[5][0]                    \n                                                                 bidirectional[0][0]              \n                                                                 softmax[6][0]                    \n                                                                 bidirectional[0][0]              \n                                                                 softmax[7][0]                    \n                                                                 bidirectional[0][0]              \n                                                                 softmax[8][0]                    \n                                                                 bidirectional[0][0]              \n                                                                 softmax[9][0]                    \n                                                                 bidirectional[0][0]              \n__________________________________________________________________________________________________\nc0 (InputLayer)                 [(None, 64)]         0                                            \n__________________________________________________________________________________________________\nlstm (LSTM)                     [(None, 64), (None,  33024       dot[0][0]                        \n                                                                 s0[0][0]                         \n                                                                 c0[0][0]                         \n                                                                 dot[1][0]                        \n                                                                 lstm[0][0]                       \n                                                                 lstm[0][2]                       \n                                                                 dot[2][0]                        \n                                                                 lstm[1][0]                       \n                                                                 lstm[1][2]                       \n                                                                 dot[3][0]                        \n                                                                 lstm[2][0]                       \n                                                                 lstm[2][2]                       \n                                                                 dot[4][0]                        \n                                                                 lstm[3][0]                       \n                                                                 lstm[3][2]                       \n                                                                 dot[5][0]                        \n                                                                 lstm[4][0]                       \n                                                                 lstm[4][2]                       \n                                                                 dot[6][0]                        \n                                                                 lstm[5][0]                       \n                                                                 lstm[5][2]                       \n                                                                 dot[7][0]                        \n                                                                 lstm[6][0]                       \n                                                                 lstm[6][2]                       \n                                                                 dot[8][0]                        \n                                                                 lstm[7][0]                       \n                                                                 lstm[7][2]                       \n                                                                 dot[9][0]                        \n                                                                 lstm[8][0]                       \n                                                                 lstm[8][2]                       \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 11)           715         lstm[0][0]                       \n                                                                 lstm[1][0]                       \n                                                                 lstm[2][0]                       \n                                                                 lstm[3][0]                       \n                                                                 lstm[4][0]                       \n                                                                 lstm[5][0]                       \n                                                                 lstm[6][0]                       \n                                                                 lstm[7][0]                       \n                                                                 lstm[8][0]                       \n                                                                 lstm[9][0]                       \n==================================================================================================\nTotal params: 52,960\nTrainable params: 52,960\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01) # Adam(...) \n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n",
    "\n",
    "# Initializing the intitial hidden cell state (s0, usually a0 is used) and the memory cell\n",
    "# state (c0)\n",
    "s0 = np.zeros((m, n_s))\n",
    "c0 = np.zeros((m, n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))\n",
    "\n",
    "s0_val = np.zeros((m_val, n_s))\n",
    "c0_val = np.zeros((m_val, n_s))\n",
    "outputs_val = list(Yoh_val.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_logdir = os.path.join(os.curdir, \"logs\")\n",
    "\n",
    "def get_run_id(string=None):\n",
    "    \"\"\"Returns the current run id and the log dire\n",
    "    \"\"\"\n",
    "    if string:\n",
    "        run_id = \"run-\" + string + \"-\" + time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    else:\n",
    "        run_id = 'run-' + time.strftime(\"%Y-%m-%d-%H-%M\")\n",
    "    return run_id, os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tep - loss: 0.8373 - dense_2_loss: 0.0247 - dense_2_1_loss: 0.0214 - dense_2_2_loss: 0.1564 - dense_2_3_loss: 0.1172 - dense_2_4_loss: 0.0030 - dense_2_5_loss: 0.0376 - dense_2_6_loss: 0.2107 - dense_2_7_loss: 0.0059 - dense_2_8_loss: 0.0811 - dense_2_9_loss: 0.1794 - dense_2_accuracy: 0.9908 - dense_2_1_accuracy: 0.9919 - dense_2_2_accuracy: 0.9340 - dense_2_3_accuracy: 0.9796 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9883 - dense_2_6_accuracy: 0.9398 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9819 - dense_2_9_accuracy: 0.9537 - val_loss: 0.8892 - val_dense_2_loss: 0.0279 - val_dense_2_1_loss: 0.0253 - val_dense_2_2_loss: 0.1595 - val_dense_2_3_loss: 0.1149 - val_dense_2_4_loss: 0.0028 - val_dense_2_5_loss: 0.0426 - val_dense_2_6_loss: 0.2276 - val_dense_2_7_loss: 0.0065 - val_dense_2_8_loss: 0.0966 - val_dense_2_9_loss: 0.1854 - val_dense_2_accuracy: 0.9898 - val_dense_2_1_accuracy: 0.9892 - val_dense_2_2_accuracy: 0.9326 - val_dense_2_3_accuracy: 0.9750 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9852 - val_dense_2_6_accuracy: 0.9332 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9740 - val_dense_2_9_accuracy: 0.9530\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 33s 664ms/step - loss: 0.8041 - dense_2_loss: 0.0222 - dense_2_1_loss: 0.0180 - dense_2_2_loss: 0.1448 - dense_2_3_loss: 0.1117 - dense_2_4_loss: 0.0030 - dense_2_5_loss: 0.0377 - dense_2_6_loss: 0.2077 - dense_2_7_loss: 0.0058 - dense_2_8_loss: 0.0785 - dense_2_9_loss: 0.1747 - dense_2_accuracy: 0.9928 - dense_2_1_accuracy: 0.9930 - dense_2_2_accuracy: 0.9392 - dense_2_3_accuracy: 0.9801 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9878 - dense_2_6_accuracy: 0.9395 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9824 - dense_2_9_accuracy: 0.9540 - val_loss: 0.8639 - val_dense_2_loss: 0.0275 - val_dense_2_1_loss: 0.0243 - val_dense_2_2_loss: 0.1535 - val_dense_2_3_loss: 0.1113 - val_dense_2_4_loss: 0.0027 - val_dense_2_5_loss: 0.0418 - val_dense_2_6_loss: 0.2212 - val_dense_2_7_loss: 0.0062 - val_dense_2_8_loss: 0.0936 - val_dense_2_9_loss: 0.1817 - val_dense_2_accuracy: 0.9894 - val_dense_2_1_accuracy: 0.9900 - val_dense_2_2_accuracy: 0.9348 - val_dense_2_3_accuracy: 0.9758 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9856 - val_dense_2_6_accuracy: 0.9334 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9764 - val_dense_2_9_accuracy: 0.9516\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 33s 660ms/step - loss: 0.7745 - dense_2_loss: 0.0222 - dense_2_1_loss: 0.0185 - dense_2_2_loss: 0.1421 - dense_2_3_loss: 0.1061 - dense_2_4_loss: 0.0028 - dense_2_5_loss: 0.0349 - dense_2_6_loss: 0.1992 - dense_2_7_loss: 0.0054 - dense_2_8_loss: 0.0767 - dense_2_9_loss: 0.1665 - dense_2_accuracy: 0.9917 - dense_2_1_accuracy: 0.9929 - dense_2_2_accuracy: 0.9406 - dense_2_3_accuracy: 0.9822 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9895 - dense_2_6_accuracy: 0.9438 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9829 - dense_2_9_accuracy: 0.9576 - val_loss: 0.8450 - val_dense_2_loss: 0.0269 - val_dense_2_1_loss: 0.0241 - val_dense_2_2_loss: 0.1472 - val_dense_2_3_loss: 0.1097 - val_dense_2_4_loss: 0.0026 - val_dense_2_5_loss: 0.0402 - val_dense_2_6_loss: 0.2201 - val_dense_2_7_loss: 0.0058 - val_dense_2_8_loss: 0.0902 - val_dense_2_9_loss: 0.1783 - val_dense_2_accuracy: 0.9892 - val_dense_2_1_accuracy: 0.9896 - val_dense_2_2_accuracy: 0.9388 - val_dense_2_3_accuracy: 0.9774 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9860 - val_dense_2_6_accuracy: 0.9326 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9746 - val_dense_2_9_accuracy: 0.9502\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 33s 652ms/step - loss: 0.7713 - dense_2_loss: 0.0239 - dense_2_1_loss: 0.0198 - dense_2_2_loss: 0.1384 - dense_2_3_loss: 0.1041 - dense_2_4_loss: 0.0027 - dense_2_5_loss: 0.0387 - dense_2_6_loss: 0.2028 - dense_2_7_loss: 0.0056 - dense_2_8_loss: 0.0750 - dense_2_9_loss: 0.1603 - dense_2_accuracy: 0.9913 - dense_2_1_accuracy: 0.9931 - dense_2_2_accuracy: 0.9430 - dense_2_3_accuracy: 0.9828 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9872 - dense_2_6_accuracy: 0.9409 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9831 - dense_2_9_accuracy: 0.9614 - val_loss: 0.8169 - val_dense_2_loss: 0.0260 - val_dense_2_1_loss: 0.0229 - val_dense_2_2_loss: 0.1438 - val_dense_2_3_loss: 0.1046 - val_dense_2_4_loss: 0.0026 - val_dense_2_5_loss: 0.0400 - val_dense_2_6_loss: 0.2147 - val_dense_2_7_loss: 0.0059 - val_dense_2_8_loss: 0.0871 - val_dense_2_9_loss: 0.1694 - val_dense_2_accuracy: 0.9904 - val_dense_2_1_accuracy: 0.9906 - val_dense_2_2_accuracy: 0.9422 - val_dense_2_3_accuracy: 0.9782 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9850 - val_dense_2_6_accuracy: 0.9344 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9772 - val_dense_2_9_accuracy: 0.9576\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 0.7457 - dense_2_loss: 0.0229 - dense_2_1_loss: 0.0187 - dense_2_2_loss: 0.1311 - dense_2_3_loss: 0.1022 - dense_2_4_loss: 0.0028 - dense_2_5_loss: 0.0357 - dense_2_6_loss: 0.2017 - dense_2_7_loss: 0.0055 - dense_2_8_loss: 0.0692 - dense_2_9_loss: 0.1561 - dense_2_accuracy: 0.9921 - dense_2_1_accuracy: 0.9931 - dense_2_2_accuracy: 0.9484 - dense_2_3_accuracy: 0.9829 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9890 - dense_2_6_accuracy: 0.9388 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9850 - dense_2_9_accuracy: 0.9628 - val_loss: 0.8116 - val_dense_2_loss: 0.0252 - val_dense_2_1_loss: 0.0222 - val_dense_2_2_loss: 0.1392 - val_dense_2_3_loss: 0.1107 - val_dense_2_4_loss: 0.0023 - val_dense_2_5_loss: 0.0398 - val_dense_2_6_loss: 0.2164 - val_dense_2_7_loss: 0.0058 - val_dense_2_8_loss: 0.0832 - val_dense_2_9_loss: 0.1668 - val_dense_2_accuracy: 0.9902 - val_dense_2_1_accuracy: 0.9912 - val_dense_2_2_accuracy: 0.9426 - val_dense_2_3_accuracy: 0.9762 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9862 - val_dense_2_6_accuracy: 0.9334 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9780 - val_dense_2_9_accuracy: 0.9590\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 33s 665ms/step - loss: 0.7194 - dense_2_loss: 0.0208 - dense_2_1_loss: 0.0172 - dense_2_2_loss: 0.1271 - dense_2_3_loss: 0.1008 - dense_2_4_loss: 0.0026 - dense_2_5_loss: 0.0328 - dense_2_6_loss: 0.1917 - dense_2_7_loss: 0.0055 - dense_2_8_loss: 0.0684 - dense_2_9_loss: 0.1526 - dense_2_accuracy: 0.9929 - dense_2_1_accuracy: 0.9933 - dense_2_2_accuracy: 0.9477 - dense_2_3_accuracy: 0.9831 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9896 - dense_2_6_accuracy: 0.9434 - dense_2_7_accuracy: 0.9999 - dense_2_8_accuracy: 0.9848 - dense_2_9_accuracy: 0.9626 - val_loss: 0.7771 - val_dense_2_loss: 0.0248 - val_dense_2_1_loss: 0.0216 - val_dense_2_2_loss: 0.1329 - val_dense_2_3_loss: 0.0996 - val_dense_2_4_loss: 0.0023 - val_dense_2_5_loss: 0.0384 - val_dense_2_6_loss: 0.2085 - val_dense_2_7_loss: 0.0055 - val_dense_2_8_loss: 0.0799 - val_dense_2_9_loss: 0.1637 - val_dense_2_accuracy: 0.9906 - val_dense_2_1_accuracy: 0.9918 - val_dense_2_2_accuracy: 0.9452 - val_dense_2_3_accuracy: 0.9804 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9860 - val_dense_2_6_accuracy: 0.9348 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9788 - val_dense_2_9_accuracy: 0.9590\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 0.7030 - dense_2_loss: 0.0208 - dense_2_1_loss: 0.0172 - dense_2_2_loss: 0.1226 - dense_2_3_loss: 0.0944 - dense_2_4_loss: 0.0025 - dense_2_5_loss: 0.0346 - dense_2_6_loss: 0.1903 - dense_2_7_loss: 0.0049 - dense_2_8_loss: 0.0642 - dense_2_9_loss: 0.1515 - dense_2_accuracy: 0.9928 - dense_2_1_accuracy: 0.9935 - dense_2_2_accuracy: 0.9506 - dense_2_3_accuracy: 0.9850 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9887 - dense_2_6_accuracy: 0.9428 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9872 - dense_2_9_accuracy: 0.9632 - val_loss: 0.7552 - val_dense_2_loss: 0.0246 - val_dense_2_1_loss: 0.0216 - val_dense_2_2_loss: 0.1306 - val_dense_2_3_loss: 0.0973 - val_dense_2_4_loss: 0.0022 - val_dense_2_5_loss: 0.0370 - val_dense_2_6_loss: 0.2027 - val_dense_2_7_loss: 0.0051 - val_dense_2_8_loss: 0.0747 - val_dense_2_9_loss: 0.1595 - val_dense_2_accuracy: 0.9914 - val_dense_2_1_accuracy: 0.9914 - val_dense_2_2_accuracy: 0.9446 - val_dense_2_3_accuracy: 0.9812 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9862 - val_dense_2_6_accuracy: 0.9366 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9804 - val_dense_2_9_accuracy: 0.9600\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 32s 637ms/step - loss: 0.6896 - dense_2_loss: 0.0200 - dense_2_1_loss: 0.0157 - dense_2_2_loss: 0.1209 - dense_2_3_loss: 0.0978 - dense_2_4_loss: 0.0025 - dense_2_5_loss: 0.0317 - dense_2_6_loss: 0.1868 - dense_2_7_loss: 0.0050 - dense_2_8_loss: 0.0640 - dense_2_9_loss: 0.1452 - dense_2_accuracy: 0.9927 - dense_2_1_accuracy: 0.9944 - dense_2_2_accuracy: 0.9505 - dense_2_3_accuracy: 0.9849 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9897 - dense_2_6_accuracy: 0.9440 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9862 - dense_2_9_accuracy: 0.9661 - val_loss: 0.7385 - val_dense_2_loss: 0.0235 - val_dense_2_1_loss: 0.0204 - val_dense_2_2_loss: 0.1260 - val_dense_2_3_loss: 0.0942 - val_dense_2_4_loss: 0.0021 - val_dense_2_5_loss: 0.0363 - val_dense_2_6_loss: 0.2024 - val_dense_2_7_loss: 0.0045 - val_dense_2_8_loss: 0.0741 - val_dense_2_9_loss: 0.1550 - val_dense_2_accuracy: 0.9912 - val_dense_2_1_accuracy: 0.9924 - val_dense_2_2_accuracy: 0.9490 - val_dense_2_3_accuracy: 0.9834 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9870 - val_dense_2_6_accuracy: 0.9368 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9816 - val_dense_2_9_accuracy: 0.9602\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 33s 667ms/step - loss: 0.6601 - dense_2_loss: 0.0196 - dense_2_1_loss: 0.0156 - dense_2_2_loss: 0.1123 - dense_2_3_loss: 0.0890 - dense_2_4_loss: 0.0023 - dense_2_5_loss: 0.0314 - dense_2_6_loss: 0.1825 - dense_2_7_loss: 0.0047 - dense_2_8_loss: 0.0620 - dense_2_9_loss: 0.1406 - dense_2_accuracy: 0.9935 - dense_2_1_accuracy: 0.9941 - dense_2_2_accuracy: 0.9571 - dense_2_3_accuracy: 0.9867 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9901 - dense_2_6_accuracy: 0.9459 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9879 - dense_2_9_accuracy: 0.9668 - val_loss: 0.7246 - val_dense_2_loss: 0.0230 - val_dense_2_1_loss: 0.0196 - val_dense_2_2_loss: 0.1247 - val_dense_2_3_loss: 0.0940 - val_dense_2_4_loss: 0.0023 - val_dense_2_5_loss: 0.0354 - val_dense_2_6_loss: 0.1984 - val_dense_2_7_loss: 0.0048 - val_dense_2_8_loss: 0.0716 - val_dense_2_9_loss: 0.1507 - val_dense_2_accuracy: 0.9910 - val_dense_2_1_accuracy: 0.9926 - val_dense_2_2_accuracy: 0.9504 - val_dense_2_3_accuracy: 0.9844 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9878 - val_dense_2_6_accuracy: 0.9398 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9830 - val_dense_2_9_accuracy: 0.9616\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 33s 656ms/step - loss: 0.6484 - dense_2_loss: 0.0182 - dense_2_1_loss: 0.0141 - dense_2_2_loss: 0.1092 - dense_2_3_loss: 0.0864 - dense_2_4_loss: 0.0024 - dense_2_5_loss: 0.0306 - dense_2_6_loss: 0.1841 - dense_2_7_loss: 0.0047 - dense_2_8_loss: 0.0601 - dense_2_9_loss: 0.1386 - dense_2_accuracy: 0.9946 - dense_2_1_accuracy: 0.9950 - dense_2_2_accuracy: 0.9562 - dense_2_3_accuracy: 0.9876 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9908 - dense_2_6_accuracy: 0.9428 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9884 - dense_2_9_accuracy: 0.9664 - val_loss: 0.7324 - val_dense_2_loss: 0.0235 - val_dense_2_1_loss: 0.0201 - val_dense_2_2_loss: 0.1225 - val_dense_2_3_loss: 0.0948 - val_dense_2_4_loss: 0.0024 - val_dense_2_5_loss: 0.0387 - val_dense_2_6_loss: 0.2042 - val_dense_2_7_loss: 0.0049 - val_dense_2_8_loss: 0.0712 - val_dense_2_9_loss: 0.1501 - val_dense_2_accuracy: 0.9910 - val_dense_2_1_accuracy: 0.9916 - val_dense_2_2_accuracy: 0.9504 - val_dense_2_3_accuracy: 0.9844 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9864 - val_dense_2_6_accuracy: 0.9346 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9824 - val_dense_2_9_accuracy: 0.9624\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 32s 645ms/step - loss: 0.6409 - dense_2_loss: 0.0193 - dense_2_1_loss: 0.0154 - dense_2_2_loss: 0.1098 - dense_2_3_loss: 0.0905 - dense_2_4_loss: 0.0022 - dense_2_5_loss: 0.0307 - dense_2_6_loss: 0.1745 - dense_2_7_loss: 0.0046 - dense_2_8_loss: 0.0591 - dense_2_9_loss: 0.1348 - dense_2_accuracy: 0.9936 - dense_2_1_accuracy: 0.9944 - dense_2_2_accuracy: 0.9565 - dense_2_3_accuracy: 0.9871 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9896 - dense_2_6_accuracy: 0.9460 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9875 - dense_2_9_accuracy: 0.9696 - val_loss: 0.6881 - val_dense_2_loss: 0.0221 - val_dense_2_1_loss: 0.0187 - val_dense_2_2_loss: 0.1158 - val_dense_2_3_loss: 0.0884 - val_dense_2_4_loss: 0.0020 - val_dense_2_5_loss: 0.0337 - val_dense_2_6_loss: 0.1911 - val_dense_2_7_loss: 0.0044 - val_dense_2_8_loss: 0.0663 - val_dense_2_9_loss: 0.1456 - val_dense_2_accuracy: 0.9918 - val_dense_2_1_accuracy: 0.9930 - val_dense_2_2_accuracy: 0.9544 - val_dense_2_3_accuracy: 0.9856 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9876 - val_dense_2_6_accuracy: 0.9392 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9842 - val_dense_2_9_accuracy: 0.9650\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 33s 657ms/step - loss: 0.6229 - dense_2_loss: 0.0178 - dense_2_1_loss: 0.0137 - dense_2_2_loss: 0.1043 - dense_2_3_loss: 0.0840 - dense_2_4_loss: 0.0022 - dense_2_5_loss: 0.0288 - dense_2_6_loss: 0.1744 - dense_2_7_loss: 0.0043 - dense_2_8_loss: 0.0575 - dense_2_9_loss: 0.1359 - dense_2_accuracy: 0.9938 - dense_2_1_accuracy: 0.9954 - dense_2_2_accuracy: 0.9615 - dense_2_3_accuracy: 0.9888 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9906 - dense_2_6_accuracy: 0.9465 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9865 - dense_2_9_accuracy: 0.9672 - val_loss: 0.6924 - val_dense_2_loss: 0.0219 - val_dense_2_1_loss: 0.0187 - val_dense_2_2_loss: 0.1165 - val_dense_2_3_loss: 0.0916 - val_dense_2_4_loss: 0.0023 - val_dense_2_5_loss: 0.0352 - val_dense_2_6_loss: 0.1910 - val_dense_2_7_loss: 0.0049 - val_dense_2_8_loss: 0.0664 - val_dense_2_9_loss: 0.1437 - val_dense_2_accuracy: 0.9920 - val_dense_2_1_accuracy: 0.9930 - val_dense_2_2_accuracy: 0.9532 - val_dense_2_3_accuracy: 0.9850 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9868 - val_dense_2_6_accuracy: 0.9422 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9826 - val_dense_2_9_accuracy: 0.9648\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 32s 638ms/step - loss: 0.6091 - dense_2_loss: 0.0167 - dense_2_1_loss: 0.0127 - dense_2_2_loss: 0.1026 - dense_2_3_loss: 0.0837 - dense_2_4_loss: 0.0022 - dense_2_5_loss: 0.0299 - dense_2_6_loss: 0.1710 - dense_2_7_loss: 0.0045 - dense_2_8_loss: 0.0563 - dense_2_9_loss: 0.1296 - dense_2_accuracy: 0.9947 - dense_2_1_accuracy: 0.9958 - dense_2_2_accuracy: 0.9609 - dense_2_3_accuracy: 0.9878 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9902 - dense_2_6_accuracy: 0.9486 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9883 - dense_2_9_accuracy: 0.9702 - val_loss: 0.6618 - val_dense_2_loss: 0.0210 - val_dense_2_1_loss: 0.0175 - val_dense_2_2_loss: 0.1091 - val_dense_2_3_loss: 0.0836 - val_dense_2_4_loss: 0.0020 - val_dense_2_5_loss: 0.0332 - val_dense_2_6_loss: 0.1876 - val_dense_2_7_loss: 0.0043 - val_dense_2_8_loss: 0.0643 - val_dense_2_9_loss: 0.1391 - val_dense_2_accuracy: 0.9922 - val_dense_2_1_accuracy: 0.9940 - val_dense_2_2_accuracy: 0.9576 - val_dense_2_3_accuracy: 0.9868 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9876 - val_dense_2_6_accuracy: 0.9398 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9844 - val_dense_2_9_accuracy: 0.9646\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 35s 712ms/step - loss: 0.5897 - dense_2_loss: 0.0168 - dense_2_1_loss: 0.0128 - dense_2_2_loss: 0.0982 - dense_2_3_loss: 0.0832 - dense_2_4_loss: 0.0021 - dense_2_5_loss: 0.0276 - dense_2_6_loss: 0.1690 - dense_2_7_loss: 0.0044 - dense_2_8_loss: 0.0528 - dense_2_9_loss: 0.1228 - dense_2_accuracy: 0.9951 - dense_2_1_accuracy: 0.9958 - dense_2_2_accuracy: 0.9646 - dense_2_3_accuracy: 0.9889 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9908 - dense_2_6_accuracy: 0.9485 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9896 - dense_2_9_accuracy: 0.9707 - val_loss: 0.6588 - val_dense_2_loss: 0.0211 - val_dense_2_1_loss: 0.0175 - val_dense_2_2_loss: 0.1072 - val_dense_2_3_loss: 0.0846 - val_dense_2_4_loss: 0.0021 - val_dense_2_5_loss: 0.0325 - val_dense_2_6_loss: 0.1872 - val_dense_2_7_loss: 0.0041 - val_dense_2_8_loss: 0.0650 - val_dense_2_9_loss: 0.1374 - val_dense_2_accuracy: 0.9922 - val_dense_2_1_accuracy: 0.9930 - val_dense_2_2_accuracy: 0.9584 - val_dense_2_3_accuracy: 0.9880 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9886 - val_dense_2_6_accuracy: 0.9392 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9834 - val_dense_2_9_accuracy: 0.9642\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 34s 671ms/step - loss: 0.5848 - dense_2_loss: 0.0169 - dense_2_1_loss: 0.0133 - dense_2_2_loss: 0.0949 - dense_2_3_loss: 0.0786 - dense_2_4_loss: 0.0021 - dense_2_5_loss: 0.0288 - dense_2_6_loss: 0.1642 - dense_2_7_loss: 0.0042 - dense_2_8_loss: 0.0555 - dense_2_9_loss: 0.1263 - dense_2_accuracy: 0.9945 - dense_2_1_accuracy: 0.9949 - dense_2_2_accuracy: 0.9678 - dense_2_3_accuracy: 0.9908 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9901 - dense_2_6_accuracy: 0.9503 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9879 - dense_2_9_accuracy: 0.9690 - val_loss: 0.6373 - val_dense_2_loss: 0.0201 - val_dense_2_1_loss: 0.0167 - val_dense_2_2_loss: 0.1036 - val_dense_2_3_loss: 0.0796 - val_dense_2_4_loss: 0.0021 - val_dense_2_5_loss: 0.0323 - val_dense_2_6_loss: 0.1819 - val_dense_2_7_loss: 0.0044 - val_dense_2_8_loss: 0.0610 - val_dense_2_9_loss: 0.1356 - val_dense_2_accuracy: 0.9928 - val_dense_2_1_accuracy: 0.9940 - val_dense_2_2_accuracy: 0.9600 - val_dense_2_3_accuracy: 0.9876 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9886 - val_dense_2_6_accuracy: 0.9422 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9848 - val_dense_2_9_accuracy: 0.9658\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 34s 688ms/step - loss: 0.5729 - dense_2_loss: 0.0167 - dense_2_1_loss: 0.0127 - dense_2_2_loss: 0.0930 - dense_2_3_loss: 0.0770 - dense_2_4_loss: 0.0020 - dense_2_5_loss: 0.0271 - dense_2_6_loss: 0.1627 - dense_2_7_loss: 0.0043 - dense_2_8_loss: 0.0516 - dense_2_9_loss: 0.1258 - dense_2_accuracy: 0.9945 - dense_2_1_accuracy: 0.9962 - dense_2_2_accuracy: 0.9678 - dense_2_3_accuracy: 0.9904 - dense_2_4_accuracy: 1.0000 - dense_2_5_accuracy: 0.9917 - dense_2_6_accuracy: 0.9496 - dense_2_7_accuracy: 1.0000 - dense_2_8_accuracy: 0.9892 - dense_2_9_accuracy: 0.9697 - val_loss: 0.6279 - val_dense_2_loss: 0.0196 - val_dense_2_1_loss: 0.0162 - val_dense_2_2_loss: 0.1008 - val_dense_2_3_loss: 0.0795 - val_dense_2_4_loss: 0.0020 - val_dense_2_5_loss: 0.0331 - val_dense_2_6_loss: 0.1815 - val_dense_2_7_loss: 0.0043 - val_dense_2_8_loss: 0.0591 - val_dense_2_9_loss: 0.1317 - val_dense_2_accuracy: 0.9930 - val_dense_2_1_accuracy: 0.9940 - val_dense_2_2_accuracy: 0.9622 - val_dense_2_3_accuracy: 0.9888 - val_dense_2_4_accuracy: 1.0000 - val_dense_2_5_accuracy: 0.9880 - val_dense_2_6_accuracy: 0.9422 - val_dense_2_7_accuracy: 1.0000 - val_dense_2_8_accuracy: 0.9866 - val_dense_2_9_accuracy: 0.9648\n"
     ]
    }
   ],
   "source": [
    "run_id, run_logdir = get_run_id()\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "history = model.fit([Xoh, s0, c0],\n",
    "                    outputs, \n",
    "                    epochs=50,\n",
    "                    batch_size=500,\n",
    "                    validation_data=([Xoh_val, s0_val, c0_val], outputs_val),\n",
    "                    callbacks=[tensorboard_cb]\n",
    "                    )\n",
    "model.save(run_id + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reusing TensorBoard on port 6006 (pid 39336), started 1:05:14 ago. (Use '!kill 39336' to kill it.)"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./logs --port 6006 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "source: aug 18 1997\n",
      "output: 1997-08-18 \n",
      "\n",
      "source: 5th dec 1972\n",
      "output: 1972-12-05 \n",
      "\n",
      "source: monday 10th of Jan 1992\n",
      "output: 1992-01-11 \n",
      "\n",
      "source: 20th day of mar 2016\n",
      "output: 2016-03-20 \n",
      "\n",
      "source: may 4th 1992\n",
      "output: 1992-05-04 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = ['aug 18 1997', '5th dec 1972', 'monday 10th of Jan 1992', '20th day of mar 2016', 'may 4th 1992']\n",
    "\n",
    "s00 = np.zeros((1, n_s))\n",
    "c00 = np.zeros((1, n_s))\n",
    "for example in test:\n",
    "    inputs = string_to_index(example, Tx, human_vocab)\n",
    "    inputs = np.array(list(map(lambda x: tf.keras.utils.to_categorical(x, num_classes=len(human_vocab)), inputs))).swapaxes(0,1)\n",
    "    inputs = np.swapaxes(inputs, 0, 1)\n",
    "    inputs = np.expand_dims(inputs, axis=0)\n",
    "    prediction = model.predict([inputs, s00, c00])\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    output = [machine_vocab_inv[int(i)] for i in prediction]\n",
    "    print(\"source:\", example)\n",
    "    print(\"output:\", ''.join(output),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}